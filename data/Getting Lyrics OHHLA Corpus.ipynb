{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to get a catalogue of URLs of song lyrics from the OHHLA corpus. The algorithm walks through the link structure of the website to first get all artists then their albums and then the texts of songs on the albums. Then the text items of the websites containg the lyrics are read out and the lyrics are filtered out.  \n",
    "This notebook produces a txt file which holds a list of the aquired song lyrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "import requests\n",
    "import urllib.error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlinks(url,filters=[]):\n",
    "    \n",
    "    newurl=[]\n",
    "    #brokenlist=[]\n",
    "\n",
    "    for item in url:\n",
    "    \n",
    "        #some links on the website are broken and can be skipped\n",
    "        try:\n",
    "            html_page = urlopen(item)\n",
    "        except urllib.error.HTTPError as e:\n",
    "            if e.getcode() == 404: # check the return code\n",
    "                #brokenlist.append(item)\n",
    "                continue\n",
    "            raise \n",
    "\n",
    "        soup = BeautifulSoup(html_page, \"lxml\")\n",
    "\n",
    "\n",
    "        for link in soup.findAll('a'):\n",
    "\n",
    "            #filters out any links that dont redirect to albums\n",
    "            if any(t in str(link) for t in filters) or link.get(\"href\") is None:\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "\n",
    "                newurl.append(item+link.get('href'))\n",
    "                \n",
    "    return newurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Walks through OHHLA.com and makes a list of links to all websites with lyrics listed on the site. \n",
    "Then reads out the text of the website to get the lyrics\"\"\"\n",
    "def getlyrics(starturl,baseurl,notalbums,size):\n",
    "\n",
    "    #the rl redirects to a list of all artist listed on ohhla\n",
    "    req=Request(starturl)\n",
    "    html_page = urlopen(req)\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "\n",
    "    #filters out some links that dont belong to artists\n",
    "    filters=[\"YFA\",\"anonymous//\"]\n",
    "\n",
    "    #get all links from the website and filters out some links directing elsewhere\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "\n",
    "        if any(q in str(link) for q in filters) or link.get('href') is None:\n",
    "            pass\n",
    "        else:    \n",
    "            links.append(baseurl+link.get('href'))\n",
    "\n",
    "    \n",
    "    #some links at the star and at the end of the website dont direct to artists and are cut\n",
    "    end=len(links)-2\n",
    "    artlinks=links[size:end]\n",
    "    \n",
    "    \n",
    "    #Call getlinks to obtain the links leading to the albums listed for the artists\n",
    "    albumurl=getlinks(artlinks,notalbums)\n",
    "\n",
    "   \n",
    "    \n",
    "    #Call getlinks to obtain the links leading to the lyrics listed for one album\n",
    "    txturls=getlinks(albumurl,notalbums)\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    textall=[]\n",
    "\n",
    "    for l in txturls:\n",
    "        res = requests.get(l)\n",
    "        html_page = res.content\n",
    "        soup = BeautifulSoup(html_page, 'html.parser')\n",
    "        text = soup.find_all(text=True)\n",
    "        \n",
    "\n",
    "        #in the returned list the lyrics are always the longest item \n",
    "        #this filters out only the lyrics from all the text items on the website\n",
    "        textall.append(max(text, key=len))\n",
    "    return textall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Driver code\n",
    "#The artists in the corpus are listed over five websites with different url\n",
    "starturl=[\"http://ohhla.com/all.html\",\"http://ohhla.com/all_two.html\",\n",
    "          \"http://ohhla.com/all_three.html\",\"http://ohhla.com/all_four.html\",\n",
    "         \"http://ohhla.com/all_five.html\"]\n",
    "notalbums=[\"/anonymous/\",\"?C=N;O=D\",\"?C=M;O=A\",\"?C=S;O=A\",\"?C=D;O=A\"]\n",
    "\n",
    "baseurl=\"http://ohhla.com/\"\n",
    "textall=[]\n",
    "#This is needed to filter out only links to artist on the site the number of non relevant links is different for each url\n",
    "sizes=[31,27,27,27,28]\n",
    "for (url, size) in zip(starturl, sizes):\n",
    "    textall.append(getlyrics(url,baseurl,notalbums,size))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in textall for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print function to look at the output\n",
    "for i in textall:\n",
    "    print()\n",
    "    print()\n",
    "    if i =='\\n':\n",
    "        pass\n",
    "    else:\n",
    "        print(\"item\",i)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the lyrics to a file\n",
    "target = open('lyricsall.txt', 'wb')\n",
    "for item in flat_list:\n",
    "    target.write(item.encode('ascii', 'ignore'))\n",
    "target.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(textall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
