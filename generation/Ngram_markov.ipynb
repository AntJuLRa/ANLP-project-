{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text generation using an n-gram markov model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk import ngrams\n",
    "import itertools\n",
    "import re\n",
    "import random\n",
    "import dill\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'n-gram_markov.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.load_session(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**some data preprocessing**  \n",
    "Transforming our lsit of songtexts with song name and artist tags into a list of lists of lines for each song.\n",
    "Also removing punctuation and adding Tokens for the beginning and ending of lines verses and songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('output.txt', sep='\\n\\n\\n', engine='python',encoding='utf8', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "linelist= data[\"Artist: 40 Thevz f/ Malika\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [x for x in linelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [x for x in lines if x[:5] != 'Album' and x[:6]!='Artist' and x[0]!='*' and x[:5]!='Typed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_verse(x):\n",
    "    if x[:5]=='Verse' or x[:6]=='Chorus' or x[:5]=='Intro' or x[0]=='[':\n",
    "        return \"NXTVRSE\"\n",
    "    if x[:4]=='Song':\n",
    "        return \"NXTSNG\"\n",
    "    else: \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [replace_verse(x) for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-d6601ae3a71a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmini_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[^A-Za-z0-9 -\\']\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-162-d6601ae3a71a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmini_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[^A-Za-z0-9 -\\']\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mini_lines = [re.sub(\"[^A-Za-z0-9 -\\']\", \"\", x.lower()) for x in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We either chose TreebankWordTokenizer or simple split to tokenize our lines into lists of tokens\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lines = [x.split(\" \") for x in mini_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lines = [x for x in word_lines if len(x)>0]\n",
    "for y in all_lines:\n",
    "    if y[0]!='nxtsng' and y[0]!='nxtvrse':\n",
    "        y.append('endline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "long = [x for y in all_lines for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = len(long) \n",
    "idx_list = [idx + 1 for idx, val in enumerate(long) if val == 'nxtsng'] \n",
    "  \n",
    "  \n",
    "listlist = [long[i: j] for i, j in\n",
    "        zip([0] + idx_list, idx_list + \n",
    "        ([size] if idx_list[-1] != size else []))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle the pre processed data \n",
    "pickle.dump(listlist, open( \"OHHLAdata_list.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plain n-gram markov model for generation**\n",
    "For each n-gram learn the frequency distribution of follow up words from the corpus with the train function. Then generate using the markov chain by providing a start sequence and the required of lines to the generate_lines function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ngram_markov_generator(object):\n",
    "    \n",
    "    def __init__(self, order,  end='nxtsng', endline='endline', meta_list=['nxtvrse','nxtsng']):\n",
    "        self.end = end\n",
    "        self.endline = endline\n",
    "        self.meta_list = meta_list\n",
    "        self.order =order\n",
    "        self.freq_dict = dict()\n",
    "        \n",
    "    def train(self, tknzd_txt_list):\n",
    "        for text in tknzd_txt_list:\n",
    "            grams = list(ngrams(text, self.order+1))\n",
    "            for gram in grams:\n",
    "                self.add_to_dict(gram)\n",
    "            \n",
    "    def add_to_dict(self, gram):\n",
    "        try:\n",
    "            self.freq_dict[gram[:-1]][gram[-1]]+=1\n",
    "        except KeyError:\n",
    "            self.freq_dict[gram[:-1]]= FreqDist([gram[-1]])\n",
    "    \n",
    "    def generate_text(self, start, max_len=20, temp=1):\n",
    "        key = start[-self.order:]\n",
    "        res_sent= start\n",
    "        \n",
    "        for _ in itertools.repeat(None, max_len):\n",
    "            \n",
    "            with_temp = {key: value**(1/temp) for key, value in self.freq_dict[tuple(key)].items()}\n",
    "            dist = nltk.DictionaryProbDist(with_temp,normalize=True)\n",
    "            \n",
    "            nextword = dist.generate()\n",
    "            res_sent.append(nextword)\n",
    "            \n",
    "            if nextword==self.end:\n",
    "                break\n",
    "                \n",
    "            key =res_sent[-self.order:]\n",
    "        \n",
    "        return res_sent\n",
    "    \n",
    "    def generate_lines(self, start, num_lines, max_len=200, temp=1):\n",
    "        key = start[-self.order:]\n",
    "        res_sent= start\n",
    "        linecount=0\n",
    "        for _ in itertools.repeat(None, max_len):\n",
    "            \n",
    "            with_temp = {key: value**(1/temp) for key, value in self.freq_dict[tuple(key)].items()}\n",
    "            dist = nltk.DictionaryProbDist(with_temp,normalize=True)\n",
    "            \n",
    "            nextword = dist.generate()\n",
    "            res_sent.append(nextword)\n",
    "            \n",
    "            if nextword==self.endline:\n",
    "                linecount = linecount + 1\n",
    "            \n",
    "            if linecount >= num_lines:\n",
    "                break\n",
    "            if nextword==self.end:\n",
    "                break\n",
    "                \n",
    "            key =res_sent[-self.order:]\n",
    "        \n",
    "        return res_sent\n",
    "    def nice_format(self, output_list):\n",
    "        no_meta = [x for x in output_list if x not in self.meta_list]\n",
    "        with_linebreaks = [\"\\n\" if x==self.endline else x for x in no_meta]\n",
    "        return \" \".join(with_linebreaks)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_mc = ngram_markov_generator(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_mc.train(listlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was so young \n",
      " cause fire then you're next \n",
      " you better check your boy now \n",
      " don't be wrong yeah your\n"
     ]
    }
   ],
   "source": [
    "text= trigram_mc.generate_text(['nxtvrse','i','was','so'],max_len=20,temp=1)\n",
    "print(trigram_mc.nice_format(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was so devastating i feel like \n",
      " drum tap young cat wit 9 lives from the '80s this world '' \n",
      " i promise \n",
      " i do n't admit it \n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines= trigram_mc.generate_lines(['nxtvrse','i','was','so'],4)\n",
    "print(trigram_mc.nice_format(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating 100 4-liners for evaluation and classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_list= []\n",
    "for x in range(100):\n",
    "  beginning= random.choice(listlist)\n",
    "  mc_list.append(trigram_mc.nice_format(trigram_mc.generate_lines(beginning[0:2],4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mc_list, open(\"mc_list.p\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
