{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Ngram_markov.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMFrqr5GAlrx"
      },
      "source": [
        "**Text generation using an n-gram markov model**  \r\n",
        "ANLP 2020/2021 final project  \r\n",
        "Friederike Schreiber, Peng Chen, Anton Rabe\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA897eYQAltz"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import FreqDist\n",
        "from nltk import ngrams\n",
        "import itertools\n",
        "import re\n",
        "import random\n",
        "import dill\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUe2Uz1dAlt-"
      },
      "source": [
        "filename = '../resources/n-gram_markov.pkl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSqdXUMsAluG"
      },
      "source": [
        "dill.dump_session(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0YwChqOAluM"
      },
      "source": [
        "dill.load_session(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7GWLrqIAluS"
      },
      "source": [
        "**some data preprocessing**  \n",
        "Transforming our lsit of songtexts with song name and artist tags into a list of lists of lines for each song.\n",
        "Also removing punctuation and adding Tokens for the beginning and ending of lines verses and songs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxgARCo4AluZ"
      },
      "source": [
        "data = pd.read_csv('../resources/output.txt', sep='\\n\\n\\n', engine='python',encoding='utf8', header = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a80IPuwxAlud"
      },
      "source": [
        "linelist= data[\"Artist: 40 Thevz f/ Malika\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-7veFg8Alug"
      },
      "source": [
        "lines = [x for x in linelist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M15Sc5XMAluk"
      },
      "source": [
        "lines = [x for x in lines if x[:5] != 'Album' and x[:6]!='Artist' and x[0]!='*' and x[:5]!='Typed']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he1s06M4Aluu"
      },
      "source": [
        "def replace_verse(x):\n",
        "    if x[:5]=='Verse' or x[:6]=='Chorus' or x[:5]=='Intro' or x[0]=='[':\n",
        "        return \"NXTVRSE\"\n",
        "    if x[:4]=='Song':\n",
        "        return \"NXTSNG\"\n",
        "    else: \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QbfLj3YAlu6"
      },
      "source": [
        "lines = [replace_verse(x) for x in lines]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-Uzo9S8AlvD",
        "outputId": "cd7ebca8-d215-4285-8dbf-c1830781f8ef"
      },
      "source": [
        "mini_lines = [re.sub(\"[^A-Za-z0-9 -\\']\", \"\", x.lower()) for x in lines]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MemoryError",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-162-d6601ae3a71a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmini_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[^A-Za-z0-9 -\\']\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-162-d6601ae3a71a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmini_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[^A-Za-z0-9 -\\']\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8QSVX8vAlvQ"
      },
      "source": [
        "#We either chose TreebankWordTokenizer or simple split to tokenize our lines into lists of tokens\n",
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVTO7LSLAlvS"
      },
      "source": [
        "word_lines = [x.split(\" \") for x in mini_lines]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CECCd7wXAlvV"
      },
      "source": [
        "all_lines = [x for x in word_lines if len(x)>0]\n",
        "for y in all_lines:\n",
        "    if y[0]!='nxtsng' and y[0]!='nxtvrse':\n",
        "        y.append('endline')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SJsRckZAlvW"
      },
      "source": [
        "long = [x for y in all_lines for x in y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdArFzQqAlvb"
      },
      "source": [
        "size = len(long) \n",
        "idx_list = [idx + 1 for idx, val in enumerate(long) if val == 'nxtsng'] \n",
        "  \n",
        "  \n",
        "listlist = [long[i: j] for i, j in\n",
        "        zip([0] + idx_list, idx_list + \n",
        "        ([size] if idx_list[-1] != size else []))] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEusGHjdAlve"
      },
      "source": [
        "#pickle the pre processed data \n",
        "pickle.dump(listlist, open( \"../resources/OHHLAdata_list.p\", \"wb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f62LOZ_aAlvh"
      },
      "source": [
        "**Plain n-gram markov model for generation**\n",
        "For each n-gram learn the frequency distribution of follow up words from the corpus with the train function. Then generate using the markov chain by providing a start sequence and the required of lines to the generate_lines function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOxDeHObAlvk"
      },
      "source": [
        "class ngram_markov_generator(object):\n",
        "    \n",
        "    def __init__(self, order,  end='nxtsng', endline='endline', meta_list=['nxtvrse','nxtsng']):\n",
        "        self.end = end\n",
        "        self.endline = endline\n",
        "        self.meta_list = meta_list\n",
        "        self.order =order\n",
        "        self.freq_dict = dict()\n",
        "        \n",
        "    def train(self, tknzd_txt_list):\n",
        "        for text in tknzd_txt_list:\n",
        "            grams = list(ngrams(text, self.order+1))\n",
        "            for gram in grams:\n",
        "                self.add_to_dict(gram)\n",
        "            \n",
        "    def add_to_dict(self, gram):\n",
        "        try:\n",
        "            self.freq_dict[gram[:-1]][gram[-1]]+=1\n",
        "        except KeyError:\n",
        "            self.freq_dict[gram[:-1]]= FreqDist([gram[-1]])\n",
        "    \n",
        "    def generate_text(self, start, max_len=20, temp=1):\n",
        "        key = start[-self.order:]\n",
        "        res_sent= start\n",
        "        \n",
        "        for _ in itertools.repeat(None, max_len):\n",
        "            \n",
        "            with_temp = {key: value**(1/temp) for key, value in self.freq_dict[tuple(key)].items()}\n",
        "            dist = nltk.DictionaryProbDist(with_temp,normalize=True)\n",
        "            \n",
        "            nextword = dist.generate()\n",
        "            res_sent.append(nextword)\n",
        "            \n",
        "            if nextword==self.end:\n",
        "                break\n",
        "                \n",
        "            key =res_sent[-self.order:]\n",
        "        \n",
        "        return res_sent\n",
        "    \n",
        "    def generate_lines(self, start, num_lines, max_len=200, temp=1):\n",
        "        key = start[-self.order:]\n",
        "        res_sent= start\n",
        "        linecount=0\n",
        "        for _ in itertools.repeat(None, max_len):\n",
        "            \n",
        "            with_temp = {key: value**(1/temp) for key, value in self.freq_dict[tuple(key)].items()}\n",
        "            dist = nltk.DictionaryProbDist(with_temp,normalize=True)\n",
        "            \n",
        "            nextword = dist.generate()\n",
        "            res_sent.append(nextword)\n",
        "            \n",
        "            if nextword==self.endline:\n",
        "                linecount = linecount + 1\n",
        "            \n",
        "            if linecount >= num_lines:\n",
        "                break\n",
        "            if nextword==self.end:\n",
        "                break\n",
        "                \n",
        "            key =res_sent[-self.order:]\n",
        "        \n",
        "        return res_sent\n",
        "    def nice_format(self, output_list):\n",
        "        no_meta = [x for x in output_list if x not in self.meta_list]\n",
        "        with_linebreaks = [\"\\n\" if x==self.endline else x for x in no_meta]\n",
        "        return \" \".join(with_linebreaks)\n",
        "        \n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-VuvKaYAlvm"
      },
      "source": [
        "trigram_mc = ngram_markov_generator(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2oQakYRAlvp"
      },
      "source": [
        "trigram_mc.train(listlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77eJPaaGAlvs",
        "outputId": "44611da5-2932-45f7-9743-ff31fe06dab2"
      },
      "source": [
        "text= trigram_mc.generate_text(['nxtvrse','i','was','so'],max_len=20,temp=1)\n",
        "print(trigram_mc.nice_format(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i was so young \n",
            " cause fire then you're next \n",
            " you better check your boy now \n",
            " don't be wrong yeah your\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ6TEUY0Alvw",
        "outputId": "cb772941-7a70-4e92-f2dc-8a0489a3b626"
      },
      "source": [
        "lines= trigram_mc.generate_lines(['nxtvrse','i','was','so'],4)\n",
        "print(trigram_mc.nice_format(lines))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i was so devastating i feel like \n",
            " drum tap young cat wit 9 lives from the '80s this world '' \n",
            " i promise \n",
            " i do n't admit it \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5c-wj8vAlv1"
      },
      "source": [
        "**Generating 100 4-liners for evaluation and classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCtFmzuAAlv2"
      },
      "source": [
        "mc_list= []\n",
        "for x in range(100):\n",
        "  beginning= random.choice(listlist)\n",
        "  mc_list.append(trigram_mc.nice_format(trigram_mc.generate_lines(beginning[0:2],4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0vjq24WAlv5"
      },
      "source": [
        "pickle.dump(mc_list, open(\"../resources/mc_list.p\",\"wb\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}