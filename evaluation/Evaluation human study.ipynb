{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANLP 2020/2021 final project    \n",
    "Friederike Schreiber, Peng Chen, Anton Rabe\n",
    "\n",
    "This skript evaluates the human study.   \n",
    "\n",
    "The basic structure of the study:\n",
    "The study was split into two parts the first part testing for the Naturalness of the given sentence. \n",
    "The second part asking if a sentence was more likely written by a human or a machine.  \n",
    "\n",
    "In each part 4 conditions were tested: \n",
    "    \n",
    "    Questions 0-10 Two Layer Neural Network\n",
    "    Questions 11-20 Markov Chain Model\n",
    "    Questions 21-30 Real Song Text\n",
    "    Questions 31-40 Random Text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Results of the evaluation:\n",
    "\n",
    "The Spearman Correlation shows a link between a verse being percieved as natural and likely written by a human.  \n",
    "\n",
    "ANOVA and Tukey Test show that all four test conditions are significantly different from each other for both halfs of the study. The only exception are the results for the two-layer network and the markov model. There is no significant difference between these two approaches.   \n",
    "There is also no significant difference between the two parts. \n",
    "\n",
    "ANOVA and Tukey Test show no significant difference in the results for people who have not much previous interaction with machine generated text and participants who were familiar with machine generated text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../resources/study_results.csv\",sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for only completed studies\n",
    "compl=data[data['Teilnahmestatus']==\"teilgenommen und beendet\"]\n",
    "\n",
    "#drop all columns with text only\n",
    "compl=compl.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all the meta data\n",
    "compl=compl.drop(['_Antwort-ID', 'Resume-Code',\"Start\",\"Datum und Zeit\",\"Teilnahmestatus\"], axis=1)\n",
    "\n",
    "#drop all questions about the participants\n",
    "onlyanswers=compl.drop(['1. How old are you?',\"Beginner (1) - Native Language (7)\",\"I dont listen to rap at all (1) - I hear a lot of rap music (7)\",\"No prior experience with machine generated text (1) - Frequent interaction with machine generated text (7)\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The scale changed for the second half of the study (mainly to avoid bias) to make sure participants noticed \n",
    "#there was an extra testquestion. Four participants didnt answer the test question right and were excluded. \n",
    "\n",
    "onlyanswers=onlyanswers[onlyanswers['7']!=1.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RealNat\n",
      "    RealNat  RandomNat  RealComp  RandomComp\n",
      "15        2          3         2           6\n",
      "RandomNat\n",
      "    RealNat  RandomNat  RealComp  RandomComp\n",
      "20        5          7         5           3\n",
      "RealComp\n",
      "   RealNat  RandomNat  RealComp  RandomComp\n",
      "8        3          1         6           7\n",
      "RandomComp\n",
      "   RealNat  RandomNat  RealComp  RandomComp\n",
      "2        4          2         3           2\n"
     ]
    }
   ],
   "source": [
    "#At the beginning of each part were test questions that should help familarise the participants with the setup and \n",
    "#make sure that they understood the task.\n",
    "#Answers below a certain value would indicate that the participant had difficulties with the task.\n",
    "#Some participants answered one of the four test question incorrectly but looking at their data \n",
    "#there is no indication that they had problems with the task overall.\n",
    "#For this reason they were kept in the evaluation. \n",
    "\n",
    "#Extract the test questions\n",
    "testquestion = onlyanswers[[\"Completely Unatural (1) - Completely Natural (7)\", \"Completely Unatural (1) - Completely Natural (7).1\",\"Written by a human (1) - Written by a machine (7)\",\"Written by a human (1) - Written by a machine (7).1\"]].copy()\n",
    "\n",
    "\n",
    "testquestion=testquestion.rename(columns={\"Completely Unatural (1) - Completely Natural (7)\": \"RealNat\", \"Completely Unatural (1) - Completely Natural (7).1\": \"RandomNat\",\"Written by a human (1) - Written by a machine (7)\":\"RealComp\",\"Written by a human (1) - Written by a machine (7).1\":\"RandomComp\"})\n",
    "print(\"RealNat\")\n",
    "print(testquestion[testquestion.RealNat <3.0])\n",
    "print(\"RandomNat\")\n",
    "print(testquestion[testquestion.RandomNat>5.0])\n",
    "print(\"RealComp\")\n",
    "print(testquestion[testquestion.RealComp >5.0])\n",
    "print(\"RandomComp\")\n",
    "print(testquestion[testquestion.RandomComp<3.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting different dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_natural(inputframe):\n",
    "    #Naturalness:\n",
    "\n",
    "    #get all column names that belong to the natural part\n",
    "    nat_col = [col for col in inputframe.columns if 'Natural' in col]\n",
    "\n",
    "    #make a new dataframe\n",
    "    natural= inputframe[nat_col].copy()\n",
    "\n",
    "    #the order of the naturalness questions\n",
    "    natorder=[30,23,7,9,25,26,12,19,8,20,3,33,28,14,36,24,37,5,11,39,35,10,2,32,38,6,13,17,40,15,34,4,29,16,18,31,21,22,1,27]\n",
    "\n",
    "    #rename columns\n",
    "    natural.columns = natural.columns[:2].tolist() + natorder\n",
    "\n",
    "    #drop the first to columns they contain the test data\n",
    "    natural=natural.drop([\"Completely Unatural (1) - Completely Natural (7)\",\"Completely Unatural (1) - Completely Natural (7).1\"],axis=1)\n",
    "\n",
    "    #sort the questions in their original order\n",
    "    natural = natural.reindex(sorted(natural.columns), axis=1)\n",
    "    return natural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural=frame_natural(onlyanswers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1    2   3   4   5   6   7   8   9   10  ...  31  32  33  34  35   36  37  \\\n",
       "0   6  5.0   2   3   4   2   3   3   3   4  ...   1   2   1   1   1  2.0   2   \n",
       "1   7  3.0   5   6   5   4   2   4   5   3  ...   1   2   2   1   1  2.0   1   \n",
       "2   3  5.0   3   3   5   4   3   4   2   4  ...   3   4   3   4   3  4.0   3   \n",
       "4   5  6.0   5   4   6   5   2   5   5   6  ...   6   2   2   2   2  5.0   2   \n",
       "5   5  5.0   5   3   3   4   4   4   3   5  ...   4   4   5   5   4  4.0   3   \n",
       "\n",
       "   38  39  40  \n",
       "0   2   2   1  \n",
       "1   1   2   1  \n",
       "2   3   4   3  \n",
       "4   2   5   2  \n",
       "5   4   4   5  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natural.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_comparison(inputframe):\n",
    "    #Comparison:\n",
    "\n",
    "    #get all column names that belong to the natural part\n",
    "    comp_col = [col for col in inputframe.columns if 'human' in col]\n",
    "\n",
    "    #make a new dataframe\n",
    "    compare= inputframe[comp_col].copy()\n",
    "\n",
    "    #the order of the naturalness questions\n",
    "    comporder=[9,18,39,28,27,7,31,10,32,20,8,40,33,1,11,25,2,38,34,24,19,26,30,35,29,36,22,23,3,21,17,13,12,6,15,14,4,16,5,37]\n",
    "\n",
    "    #rename columns\n",
    "    compare.columns = compare.columns[:2].tolist() + comporder\n",
    "\n",
    "    #drop the first to columns they contain the test data\n",
    "    compare=compare.drop([\"Written by a human (1) - Written by a machine (7)\",\"Written by a human (1) - Written by a machine (7).1\"],axis=1)\n",
    "\n",
    "    #sort the questions in their original order\n",
    "    compare = compare.reindex(sorted(compare.columns), axis=1)\n",
    "    \n",
    "    #In the second part of the study the scale flipped to avoid bias. But to compare the to parts here values we changed back. \n",
    "    #So 7 means written by a human, 1 written by a machine. \n",
    "    compare=compare.replace([1.0, 2.0, 3.0, 4.0,5.0,6.0,7.0], [7.0, 6.0, 5.0, 4.0,3.0,2.0,1.0])\n",
    "    \n",
    "    return compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare=frame_comparison(onlyanswers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    1    2    3    4    5    6    7    8    9    10  ...   31   32   33   34  \\\n",
       "0  6.0  3.0  3.0  3.0  6.0  3.0  2.0  2.0  4.0  3.0  ...  1.0  3.0  1.0  1.0   \n",
       "1  6.0  2.0  3.0  4.0  5.0  3.0  2.0  5.0  2.0  3.0  ...  2.0  1.0  1.0  1.0   \n",
       "2  5.0  3.0  5.0  6.0  5.0  6.0  5.0  6.0  6.0  4.0  ...  6.0  6.0  5.0  5.0   \n",
       "4  6.0  6.0  6.0  5.0  2.0  6.0  6.0  6.0  5.0  6.0  ...  6.0  6.0  5.0  2.0   \n",
       "5  4.0  4.0  5.0  4.0  5.0  3.0  4.0  3.0  5.0  NaN  ...  5.0  5.0  4.0  5.0   \n",
       "\n",
       "    35   36   37   38   39   40  \n",
       "0  2.0  2.0  3.0  2.0  2.0  2.0  \n",
       "1  1.0  1.0  1.0  1.0  1.0  2.0  \n",
       "2  5.0  5.0  6.0  5.0  6.0  5.0  \n",
       "4  2.0  6.0  5.0  2.0  3.0  5.0  \n",
       "5  NaN  4.0  5.0  5.0  3.0  3.0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 questions with NAN in the natural set\n",
      "There are 6 questions with NAN in the comparison set\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"There are\",natural.isna().sum().sum(),\"questions with NAN in the natural set\")\n",
    "print(\"There are\",compare.isna().sum().sum(),\"questions with NAN in the comparison set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some participants skipped questions for easier evaluation the mean of the other participants for this question is added\n",
    "natural=natural.apply(lambda x: x.fillna(int(x.mean())),axis=0)\n",
    "compare=compare.apply(lambda x: x.fillna(int(x.mean())),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_models(inputframe):\n",
    "    \n",
    "    tl=inputframe.iloc[:, : 10]\n",
    "    mc=inputframe.iloc[:, 10 :20]\n",
    "    real=inputframe.iloc[:, 20:30]\n",
    "    rand=inputframe.iloc[:, 30:40]\n",
    "    allframe=[tl,mc,real,rand]\n",
    "    return tl,mc,real,rand,allframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlnat,mcnat,realnat,randnat,allnat=split_models(natural)\n",
    "tlcom,mccom,realcom,randcom,allcom=split_models(compare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlall=pd.concat([tlnat, tlcom], axis=1)\n",
    "mcall=pd.concat([mcnat, mccom], axis=1)\n",
    "realall=pd.concat([realnat, realcom], axis=1)\n",
    "randall=pd.concat([randnat, randcom], axis=1)\n",
    "\n",
    "allsorted=[tlall,mcall,realall,randall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcall.columns=list(tlall.columns)\n",
    "realall.columns=list(tlall.columns)\n",
    "randall.columns=list(tlall.columns)\n",
    "\n",
    "condcombined=tlall.append([mcall,realall,randall])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_and_std(dataframe):\n",
    "    mean=round((dataframe.mean().mean()),2)\n",
    "    std=round((dataframe.stack().std()),2)\n",
    "    return mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean and Std for Naturalness:\n",
      "(3.64, 1.59)\n",
      "(3.58, 1.48)\n",
      "(5.3, 1.46)\n",
      "(2.51, 1.45)\n",
      "\n",
      "Mean and Std for Comparison:\n",
      "(3.8, 1.73)\n",
      "(3.91, 1.6)\n",
      "(5.55, 1.58)\n",
      "(2.79, 1.7)\n",
      "\n",
      "Mean and Std for both:\n",
      "(3.71, 1.66)\n",
      "(3.75, 1.55)\n",
      "(5.43, 1.53)\n",
      "(2.65, 1.59)\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean and Std for Naturalness:\")\n",
    "for frame in allnat:\n",
    "    print(mean_and_std(frame))\n",
    "\n",
    "print()\n",
    "print(\"Mean and Std for Comparison:\")\n",
    "for frame in allcom:\n",
    "    print(mean_and_std(frame))\n",
    "    \n",
    "print()\n",
    "print(\"Mean and Std for both:\")\n",
    "for frame in allsorted:\n",
    "    print(mean_and_std(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting ANOVA and Tukey "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a frame with the the four test conditions and their mean overall participants\n",
    "#Condition: Naturalness\n",
    "index=range(1,41)\n",
    "namena=pd.Series([\"tlna\",\"mcna\",\"randna\",\"realna\"])\n",
    "namena=namena.repeat(10)\n",
    "namena.index = index\n",
    "\n",
    "meanna=natural.mean()\n",
    "\n",
    "combnat=pd.concat([namena,meanna ], axis=1,ignore_index=True)\n",
    "\n",
    "combnat.columns = ['Method', 'Metascore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Condition: Comparison\n",
    "index=range(1,41)\n",
    "namecom=pd.Series([\"tlcom\",\"mccom\",\"randcom\",\"realcom\"])\n",
    "namecom=namecom.repeat(10)\n",
    "namecom.index = index\n",
    "\n",
    "meancom=compare.mean()\n",
    "\n",
    "combcom=pd.concat([namecom,meancom], axis=1,ignore_index=True)\n",
    "combcom.columns=[\"Method\",\"Metascore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Condition: Both combined\n",
    "index=range(1,81)\n",
    "nameall=pd.Series([\"tlnat\",\"tlcom\",\"mcnat\",\"mccom\",\"randnat\",\"randcom\",\"realnat\",\"realcom\"])\n",
    "nameall=nameall.repeat(10)\n",
    "nameall.index = index\n",
    "\n",
    "meanall=condcombined.mean(axis=1)\n",
    "meanall.index=index\n",
    "comball=pd.concat([nameall,meanall ], axis=1,ignore_index=True)\n",
    "\n",
    "comball.columns = ['Method', 'Metascore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            df     sum_sq    mean_sq          F        PR(>F)\n",
      "Method     3.0  39.957688  13.319229  68.092768  6.551074e-15\n",
      "Residual  36.0   7.041750   0.195604        NaN           NaN\n",
      "            df    sum_sq    mean_sq          F        PR(>F)\n",
      "Method     3.0  39.04025  13.013417  66.277569  9.878368e-15\n",
      "Residual  36.0   7.06850   0.196347        NaN           NaN\n",
      "            df     sum_sq    mean_sq         F        PR(>F)\n",
      "Method     7.0  79.346719  11.335246  11.39869  1.224757e-09\n",
      "Residual  72.0  71.599250   0.994434       NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "#Make models for oneway ANOVA test\n",
    "lm=ols(\"Metascore~Method\",data=combnat).fit()\n",
    "table=sm.stats.anova_lm(lm)\n",
    "print(table)\n",
    "\n",
    "lm=ols(\"Metascore~Method\",data=combcom).fit()\n",
    "table=sm.stats.anova_lm(lm)\n",
    "print(table)\n",
    "\n",
    "lm=ols(\"Metascore~Method\",data=comball).fit()\n",
    "table=sm.stats.anova_lm(lm)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The ANOVA results indicate that there is statistical significane between some of the test groups.\n",
    "#Use Tukey Test to see how the groups compare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tukeynat = pairwise_tukeyhsd(endog=combnat['Metascore'],\n",
    "                          groups=combnat['Method'],\n",
    "                          alpha=0.05)\n",
    "\n",
    "tukeycom = pairwise_tukeyhsd(endog=combcom['Metascore'],\n",
    "                          groups=combcom['Method'],\n",
    "                          alpha=0.05)\n",
    "tukeyall = pairwise_tukeyhsd(endog=comball['Metascore'],\n",
    "                          groups=comball['Method'],\n",
    "                          alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple Comparison of Means - Tukey HSD, FWER=0.05\n",
      "===================================================\n",
      "group1 group2 meandiff p-adj  lower   upper  reject\n",
      "---------------------------------------------------\n",
      "  mcna randna     1.72 0.001  1.1873  2.2527   True\n",
      "  mcna realna   -1.075 0.001 -1.6077 -0.5423   True\n",
      "  mcna   tlna     0.05   0.9 -0.4827  0.5827  False\n",
      "randna realna   -2.795 0.001 -3.3277 -2.2623   True\n",
      "randna   tlna    -1.67 0.001 -2.2027 -1.1373   True\n",
      "realna   tlna    1.125 0.001  0.5923  1.6577   True\n",
      "---------------------------------------------------\n",
      "\n",
      " Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "=====================================================\n",
      " group1  group2 meandiff p-adj  lower   upper  reject\n",
      "-----------------------------------------------------\n",
      "  mccom randcom     1.64 0.001  1.1063  2.1737   True\n",
      "  mccom realcom   -1.115 0.001 -1.6487 -0.5813   True\n",
      "  mccom   tlcom   -0.115   0.9 -0.6487  0.4187  False\n",
      "randcom realcom   -2.755 0.001 -3.2887 -2.2213   True\n",
      "randcom   tlcom   -1.755 0.001 -2.2887 -1.2213   True\n",
      "realcom   tlcom      1.0 0.001  0.4663  1.5337   True\n",
      "-----------------------------------------------------\n",
      "\n",
      " Multiple Comparison of Means - Tukey HSD, FWER=0.05  \n",
      "======================================================\n",
      " group1  group2 meandiff p-adj   lower   upper  reject\n",
      "------------------------------------------------------\n",
      "  mccom   mcnat   -0.005    0.9 -1.3971  1.3871  False\n",
      "  mccom randcom    1.655 0.0091  0.2629  3.0471   True\n",
      "  mccom randnat      1.7 0.0067  0.3079  3.0921   True\n",
      "  mccom realcom    -1.13 0.1982 -2.5221  0.2621  False\n",
      "  mccom realnat   -1.065  0.263 -2.4571  0.3271  False\n",
      "  mccom   tlcom    0.105    0.9 -1.2871  1.4971  False\n",
      "  mccom   tlnat   -0.175    0.9 -1.5671  1.2171  False\n",
      "  mcnat randcom     1.66 0.0088  0.2679  3.0521   True\n",
      "  mcnat randnat    1.705 0.0064  0.3129  3.0971   True\n",
      "  mcnat realcom   -1.125 0.2028 -2.5171  0.2671  False\n",
      "  mcnat realnat    -1.06 0.2685 -2.4521  0.3321  False\n",
      "  mcnat   tlcom     0.11    0.9 -1.2821  1.5021  False\n",
      "  mcnat   tlnat    -0.17    0.9 -1.5621  1.2221  False\n",
      "randcom randnat    0.045    0.9 -1.3471  1.4371  False\n",
      "randcom realcom   -2.785  0.001 -4.1771 -1.3929   True\n",
      "randcom realnat    -2.72  0.001 -4.1121 -1.3279   True\n",
      "randcom   tlcom    -1.55 0.0186 -2.9421 -0.1579   True\n",
      "randcom   tlnat    -1.83 0.0026 -3.2221 -0.4379   True\n",
      "randnat realcom    -2.83  0.001 -4.2221 -1.4379   True\n",
      "randnat realnat   -2.765  0.001 -4.1571 -1.3729   True\n",
      "randnat   tlcom   -1.595 0.0138 -2.9871 -0.2029   True\n",
      "randnat   tlnat   -1.875 0.0018 -3.2671 -0.4829   True\n",
      "realcom realnat    0.065    0.9 -1.3271  1.4571  False\n",
      "realcom   tlcom    1.235 0.1197 -0.1571  2.6271  False\n",
      "realcom   tlnat    0.955 0.4014 -0.4371  2.3471  False\n",
      "realnat   tlcom     1.17 0.1647 -0.2221  2.5621  False\n",
      "realnat   tlnat     0.89 0.4922 -0.5021  2.2821  False\n",
      "  tlcom   tlnat    -0.28    0.9 -1.6721  1.1121  False\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(tukeynat)\n",
    "print()\n",
    "print(tukeycom)\n",
    "print()\n",
    "print(tukeyall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spearmann Correlation between Naturalness and Written by a human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=0.5484845330623838, pvalue=4.191372566150168e-17)\n",
      "SpearmanrResult(correlation=0.6074746058123445, pvalue=1.4691614334082227e-21)\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "realcorr=stats.spearmanr(realnat.stack(),realcom.stack())\n",
    "print(realcorr)\n",
    "randcorr=stats.spearmanr(randnat.stack(),randcom.stack())\n",
    "print(randcorr)\n",
    "\n",
    "#The Spearman Correlation results suggest a statistically significant correlation between a sentence that is percieved as natural \n",
    "#and perceived as written by a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing results for different machine generated text knowledge backgrounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The participants were asked if they had previous experience with machine generated text\n",
    "#The following part investigates if people who claied to have previous knowledge answer different than those who have not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the participants:\n",
    "noknow=compl[compl['No prior experience with machine generated text (1) - Frequent interaction with machine generated text (7)']<4.0]\n",
    "moreknow=compl[compl['No prior experience with machine generated text (1) - Frequent interaction with machine generated text (7)']>4.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 questions with NAN in the no machine knowledge set\n",
      "There are 0 questions with NAN in the more machine knowledge set\n"
     ]
    }
   ],
   "source": [
    "#Get the cleaned up dataframe\n",
    "natural_noknow=frame_natural(noknow)\n",
    "compare_noknow=frame_comparison(noknow)\n",
    "\n",
    "#Combine the two halfs to make structuring easier\n",
    "noknowall=pd.concat([natural_noknow, compare_noknow], axis=1)\n",
    "\n",
    "natural_moreknow=frame_natural(moreknow)\n",
    "compare_moreknow=frame_comparison(moreknow)\n",
    "\n",
    "moreknowall=pd.concat([natural_moreknow, compare_moreknow], axis=1)\n",
    "\n",
    "print(\"There are\",noknowall.isna().sum().sum(),\"questions with NAN in the no machine knowledge set\")\n",
    "print(\"There are\",moreknowall.isna().sum().sum(),\"questions with NAN in the more machine knowledge set\")\n",
    "\n",
    "#Some participants skipped questions for easier evaluation the mean of the other participants for this question is added\n",
    "noknowall=noknowall.apply(lambda x: x.fillna(int(x.mean())),axis=0)\n",
    "moreknowall=moreknowall.apply(lambda x: x.fillna(int(x.mean())),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataframe into the four test conditions but combine the two parts\n",
    "def split_large_frame(inputframe):\n",
    "    tl=pd.concat([inputframe.iloc[:, : 10], inputframe.iloc[:, 40:50]], axis=1)\n",
    "    mc=pd.concat([inputframe.iloc[:, 10:20], inputframe.iloc[:, 50:60]], axis=1)\n",
    "    real=pd.concat([inputframe.iloc[:, 20:30], inputframe.iloc[:,60:70]], axis=1)\n",
    "    rand=pd.concat([inputframe.iloc[:, 30:40], inputframe.iloc[:, 70:80]], axis=1)\n",
    "    allframe=[tl,mc,real,rand]\n",
    "    return tl,mc,real,rand,allframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the dataframe\n",
    "tlno,mcno,realno,randno,noknowsort=split_large_frame(noknowall)\n",
    "tlmore,mcmore,realmore,randmore,moreknowsort=split_large_frame(moreknowall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.8, 1.75)\n",
      "(3.62, 1.5)\n",
      "(5.12, 1.55)\n",
      "(2.94, 1.59)\n",
      "\n",
      "(3.93, 1.55)\n",
      "(4.22, 1.47)\n",
      "(5.52, 1.56)\n",
      "(3.01, 1.71)\n"
     ]
    }
   ],
   "source": [
    "for frame in noknowsort:\n",
    "    print(mean_and_std(frame))\n",
    "\n",
    "print()\n",
    "for frame in moreknowsort:\n",
    "    print(mean_and_std(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Append the frames into one dataframe\n",
    "mcno.columns=list(tlno.columns)\n",
    "mcmore.columns=list(tlno.columns)\n",
    "realno.columns=list(tlno.columns)\n",
    "randno.columns=list(tlno.columns)\n",
    "realmore.columns=list(tlno.columns)\n",
    "randmore.columns=list(tlno.columns)\n",
    "\n",
    "combframe=tlno.append([tlmore,mcno,mcmore,realno,realmore,randno,randmore])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a dataframe with the mean score of each participant sorted by question\n",
    "namelist=[\"notl\",\"moretl\",\"nomc\",\"moremc\",\"noreal\",\"morereal\",\"norand\",\"morerand\"]\n",
    "index=range(1,69)\n",
    "name=pd.Series(namelist)\n",
    "name=name.repeat([8,9,8,9,8,9,8,9])\n",
    "name.index = index\n",
    "\n",
    "mean=combframe.mean(axis=1)\n",
    "mean.index=index\n",
    "\n",
    "\n",
    "comb=pd.concat([name,mean ], axis=1,ignore_index=True)\n",
    "\n",
    "comb.columns = ['Method', 'Metascore']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            df     sum_sq   mean_sq         F    PR(>F)\n",
      "Method     7.0  49.144651  7.020664  7.462714  0.000002\n",
      "Residual  60.0  56.445937  0.940766       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "#Calculate ANOVA\n",
    "lm=ols(\"Metascore~Method\",data=comb).fit()\n",
    "table=sm.stats.anova_lm(lm)\n",
    "print(table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Tukey\n",
    "tukey = pairwise_tukeyhsd(endog=comb['Metascore'],\n",
    "                          groups=comb['Method'],\n",
    "                          alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n",
      "========================================================\n",
      " group1   group2  meandiff p-adj   lower   upper  reject\n",
      "--------------------------------------------------------\n",
      "  moremc morerand  -1.2111 0.1593 -2.6469  0.2247  False\n",
      "  moremc morereal   1.4278 0.0523  -0.008  2.8635  False\n",
      "  moremc   moretl  -0.1111    0.9 -1.5469  1.3247  False\n",
      "  moremc     nomc  -0.3833    0.9 -1.8633  1.0966  False\n",
      "  moremc   norand  -0.9896 0.4295 -2.4695  0.4904  False\n",
      "  moremc   noreal   0.9917 0.4267 -0.4883  2.4716  False\n",
      "  moremc     notl  -0.3458    0.9 -1.8258  1.1341  False\n",
      "morerand morereal   2.6389  0.001  1.2031  4.0747   True\n",
      "morerand   moretl      1.1 0.2575 -0.3358  2.5358  False\n",
      "morerand     nomc   0.8278  0.632 -0.6522  2.3077  False\n",
      "morerand   norand   0.2215    0.9 -1.2584  1.7015  False\n",
      "morerand   noreal   2.2028  0.001  0.7228  3.6827   True\n",
      "morerand     notl   0.8653  0.586 -0.6147  2.3452  False\n",
      "morereal   moretl  -1.5389 0.0274 -2.9747 -0.1031   True\n",
      "morereal     nomc  -1.8111 0.0068 -3.2911 -0.3312   True\n",
      "morereal   norand  -2.4174  0.001 -3.8973 -0.9374   True\n",
      "morereal   noreal  -0.4361    0.9 -1.9161  1.0438  False\n",
      "morereal     notl  -1.7736 0.0086 -3.2536 -0.2937   True\n",
      "  moretl     nomc  -0.2722    0.9 -1.7522  1.2077  False\n",
      "  moretl   norand  -0.8785 0.5698 -2.3584  0.6015  False\n",
      "  moretl   noreal   1.1028 0.2901 -0.3772  2.5827  False\n",
      "  moretl     notl  -0.2347    0.9 -1.7147  1.2452  False\n",
      "    nomc   norand  -0.6062    0.9 -2.1291  0.9166  False\n",
      "    nomc   noreal    1.375 0.1056 -0.1479  2.8979  False\n",
      "    nomc     notl   0.0375    0.9 -1.4854  1.5604  False\n",
      "  norand   noreal   1.9812 0.0031  0.4584  3.5041   True\n",
      "  norand     notl   0.6437 0.8801 -0.8791  2.1666  False\n",
      "  noreal     notl  -1.3375 0.1253 -2.8604  0.1854  False\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(tukey)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
